{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525abbab",
   "metadata": {},
   "source": [
    "### DeeplabV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD for MMDetection 3.x (DeeplabV3, COCO)\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "import mmseg\n",
    "from mmengine.config import Config\n",
    "from mmseg.utils import register_all_modules\n",
    "from mmseg.apis import init_model\n",
    "\n",
    "###############################################################################\n",
    "# 1. Basic Setup\n",
    "###############################################################################\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "CONFIG_FILE = 'C:/Users/heheh/mmsegmentation/configs/deeplabv3/deeplabv3_r50-d8_4xb4-80k_coco-stuff164k-512x512.py'\n",
    "CHECKPOINT_FILE = 'C:/Users/heheh/mmsegmentation/checkpoints/deeplabv3_r50-d8_512x512_4x4_80k_coco-stuff164k_20210709_163016-88675c24.pth'\n",
    "IMAGE_DIR = \"C:/Users/heheh/val2017/val2017\"\n",
    "MASK_DIR = \"C:/Users/heheh/mmsegmentation/data/coco_stuff164k/annotations/val2017\"\n",
    "ADV_SAVE_DIR = \"C:/Users/heheh/mmsegmentation/data/semantic_adv_deeplabv3_2\"\n",
    "\n",
    "NUM_CLASSES = 171\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_WIDTH = 512\n",
    "IGNORE_INDEX = 255\n",
    "\n",
    "# COCO-Stuff label mapping\n",
    "COCO_STUFF_LABEL_MAP = {\n",
    "    0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10,\n",
    "    11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20,\n",
    "    22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30,\n",
    "    35: 31, 36: 32, 37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40,\n",
    "    46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48, 54: 49, 55: 50,\n",
    "    56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56, 62: 57, 63: 58, 64: 59, 65: 60,\n",
    "    67: 61, 70: 62, 72: 63, 73: 64, 74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70,\n",
    "    80: 71, 81: 72, 82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80,\n",
    "    92: 81, 93: 82, 94: 83, 95: 84, 96: 85, 97: 86, 98: 87, 99: 88, 100: 89, 101: 90,\n",
    "    102: 91, 103: 92, 104: 93, 105: 94, 106: 95, 107: 96, 108: 97, 109: 98, 110: 99, 111: 100,\n",
    "    112: 101, 113: 102, 114: 103, 115: 104, 116: 105, 117: 106, 118: 107, 119: 108, 120: 109, 121: 110,\n",
    "    122: 111, 123: 112, 124: 113, 125: 114, 126: 115, 127: 116, 128: 117, 129: 118, 130: 119, 131: 120,\n",
    "    132: 121, 133: 122, 134: 123, 135: 124, 136: 125, 137: 126, 138: 127, 139: 128, 140: 129, 141: 130,\n",
    "    142: 131, 143: 132, 144: 133, 145: 134, 146: 135, 147: 136, 148: 137, 149: 138, 150: 139, 151: 140,\n",
    "    152: 141, 153: 142, 154: 143, 155: 144, 156: 145, 157: 146, 158: 147, 159: 148, 160: 149, 161: 150,\n",
    "    162: 151, 163: 152, 164: 153, 165: 154, 166: 155, 167: 156, 168: 157, 169: 158, 170: 159, 171: 160,\n",
    "    172: 161, 173: 162, 174: 163, 175: 164, 176: 165, 177: 166, 178: 167, 179: 168, 180: 169, 181: 170,\n",
    "    182: 171, 255: 255\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 2. COCO Dataset (원본 사이즈 저장 기능 추가)\n",
    "###############################################################################\n",
    "def get_all_files(image_dir, mask_dir, img_exts=('.jpg', '.jpeg', '.png')):\n",
    "    \"\"\"이미지-마스크 쌍이 존재하는 파일들만 반환\"\"\"\n",
    "    files = sorted([f for f in os.listdir(image_dir) if f.endswith(img_exts)])\n",
    "    return [f for f in files if os.path.exists(os.path.join(mask_dir, f.rsplit('.', 1)[0] + '.png'))]\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, max_images=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        \n",
    "        self.file_list = get_all_files(image_dir, mask_dir)\n",
    "        if max_images:\n",
    "            self.file_list = self.file_list[:max_images]\n",
    "        \n",
    "        print(f\"[INFO] Found {len(self.file_list)} valid image-mask pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.file_list[idx]\n",
    "        img_path = os.path.join(self.image_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname.rsplit('.', 1)[0] + '.png')\n",
    "\n",
    "        # Load original image and get original size\n",
    "        original_image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_size = original_image.size  # (width, height)\n",
    "\n",
    "        # Resize for model processing\n",
    "        image = original_image.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.BILINEAR)\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "\n",
    "        # Load and resize mask\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = mask.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.NEAREST)\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "        # Apply label mapping\n",
    "        mapped_mask = np.full_like(mask, fill_value=255, dtype=np.uint8)\n",
    "        for original_label, mapped_label in COCO_STUFF_LABEL_MAP.items():\n",
    "            mapped_mask[mask == original_label] = mapped_label\n",
    "\n",
    "        # Normalize image\n",
    "        mean = np.array([123.675, 116.28, 103.53]).reshape(1, 1, 3)\n",
    "        std = np.array([58.395, 57.12, 57.375]).reshape(1, 1, 3)\n",
    "        image = (image - mean) / std\n",
    "        image = image.transpose(2, 0, 1)  # (C,H,W)\n",
    "\n",
    "        # Convert to tensors\n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        mask_tensor = torch.from_numpy(mapped_mask).long()\n",
    "        \n",
    "        return image_tensor, mask_tensor, fname, original_size\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle original_size as non-tensor\"\"\"\n",
    "    images, masks, filenames, original_sizes = zip(*batch)\n",
    "    \n",
    "    # Tensor로 변환할 것들\n",
    "    images = torch.stack(images)\n",
    "    masks = torch.stack(masks)\n",
    "    \n",
    "    # Tensor로 변환하지 않을 것들 (튜플로 유지)\n",
    "    filenames = list(filenames)\n",
    "    original_sizes = list(original_sizes)  # tuple로 유지\n",
    "    \n",
    "    return images, masks, filenames, original_sizes\n",
    "\n",
    "def get_dataloader(image_dir, mask_dir, batch_size=1, max_images=None):\n",
    "    dataset = COCODataset(image_dir, mask_dir, max_images)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=custom_collate_fn  # 추가\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "###############################################################################\n",
    "# 3. Model Loading\n",
    "###############################################################################\n",
    "def load_deeplabv3_model():\n",
    "    \"\"\"DeepLabV3 모델 로드\"\"\"\n",
    "    register_all_modules()\n",
    "    \n",
    "    cfg = Config.fromfile(CONFIG_FILE)\n",
    "    cfg.model.pretrained = None\n",
    "    cfg.model.train_cfg = None\n",
    "    \n",
    "    if \"test_cfg\" in cfg.model and cfg.model.test_cfg is not None:\n",
    "        cfg.model.test_cfg.mode = \"whole\"\n",
    "\n",
    "    model = init_model(cfg, checkpoint=CHECKPOINT_FILE, device=device)\n",
    "    model.eval()\n",
    "    print(f\"[INFO] Loaded DeepLabV3 model on device: {device}\")\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 4. PGD Attack Function\n",
    "###############################################################################\n",
    "def get_model_output(model, x, return_logits=False):\n",
    "    \"\"\"모델 출력 얻기\"\"\"\n",
    "    x = x.to(device, dtype=torch.float32, non_blocking=True)\n",
    "    with torch.no_grad():\n",
    "        outs = model(x, mode=\"tensor\")\n",
    "\n",
    "    if isinstance(outs, torch.Tensor):\n",
    "        pass\n",
    "    elif isinstance(outs, (list, tuple)) and len(outs) > 0:\n",
    "        outs = outs[0]\n",
    "    elif isinstance(outs, dict) and \"logits\" in outs:\n",
    "        outs = outs[\"logits\"]\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected output type: {type(outs)}\")\n",
    "\n",
    "    outs = F.interpolate(outs, size=(IMAGE_HEIGHT, IMAGE_WIDTH), mode=\"bilinear\", align_corners=False)\n",
    "    \n",
    "    if return_logits:\n",
    "        return outs\n",
    "    else:\n",
    "        return outs.argmax(dim=1)\n",
    "\n",
    "def pgd_attack(model, images, labels, epsilon, alpha, num_steps, ignore_index=255):\n",
    "    \"\"\"PGD 공격\"\"\"\n",
    "    images = images.clone().detach().to(device, dtype=torch.float32)\n",
    "    labels = labels.clone().detach().to(device)\n",
    "    delta = torch.zeros_like(images, requires_grad=True)\n",
    "\n",
    "    mean = torch.tensor([123.675, 116.28, 103.53]).view(1, 3, 1, 1).to(device, dtype=torch.float32)\n",
    "    std = torch.tensor([58.395, 57.12, 57.375]).view(1, 3, 1, 1).to(device, dtype=torch.float32)\n",
    "    min_vals = (0 - mean) / std\n",
    "    max_vals = (255 - mean) / std\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        outs = model(images + delta, mode=\"tensor\")\n",
    "        if isinstance(outs, list):\n",
    "            outs = outs[0]\n",
    "        elif isinstance(outs, dict) and \"logits\" in outs:\n",
    "            outs = outs[\"logits\"]\n",
    "\n",
    "        outs = F.interpolate(outs, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        loss = F.cross_entropy(outs, labels, ignore_index=ignore_index)\n",
    "        loss.backward()\n",
    "\n",
    "        grad = delta.grad.detach()\n",
    "        delta.data.add_(alpha * torch.sign(grad))\n",
    "        delta.data.clamp_(-epsilon, epsilon)\n",
    "        delta.data = torch.min(torch.max(images + delta.data, min_vals), max_vals) - images\n",
    "        delta.grad.zero_()\n",
    "        delta.requires_grad = True\n",
    "\n",
    "    adv = torch.clamp(images + delta, min_vals, max_vals)\n",
    "    return adv.detach()\n",
    "\n",
    "###############################################################################\n",
    "# 5. Image Saving Functions (원본 사이즈로 .jpg 저장)\n",
    "###############################################################################\n",
    "def denormalize_image(tensor_img):\n",
    "    \"\"\"정규화된 텐서를 원본 이미지로 변환\"\"\"\n",
    "    mean = torch.tensor([123.675, 116.28, 103.53]).view(1, 3, 1, 1).to(tensor_img.device)\n",
    "    std = torch.tensor([58.395, 57.12, 57.375]).view(1, 3, 1, 1).to(tensor_img.device)\n",
    "    \n",
    "    denorm = tensor_img * std + mean\n",
    "    denorm = torch.clamp(denorm, 0, 255)\n",
    "    return denorm\n",
    "\n",
    "def save_adversarial_image_with_original_size(adv_tensor, filename, original_size, save_dir):\n",
    "    \"\"\"Adversarial 이미지를 원본 사이즈 .jpg 형태로 저장\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Denormalize and convert to numpy\n",
    "    denorm = denormalize_image(adv_tensor)\n",
    "    img_np = denorm[0].cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "    \n",
    "    # Convert to PIL\n",
    "    img_pil = Image.fromarray(img_np)\n",
    "    \n",
    "    # Resize back to original size\n",
    "    img_pil = img_pil.resize(original_size, Image.BILINEAR)\n",
    "    \n",
    "    # Change extension to .jpg and save\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    jpg_filename = base_name + '.jpg'\n",
    "    save_path = os.path.join(save_dir, jpg_filename)\n",
    "    \n",
    "    # Save as JPEG with high quality\n",
    "    img_pil.save(save_path, 'JPEG', quality=95)\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "###############################################################################\n",
    "# 6. Evaluation Metrics\n",
    "###############################################################################\n",
    "def compute_miou(pred, target, num_classes, ignore_index=255):\n",
    "    \"\"\"mIoU 계산\"\"\"\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (pred == cls)\n",
    "        target_mask = (target == cls)\n",
    "        \n",
    "        if ignore_index is not None:\n",
    "            valid_mask = (target != ignore_index)\n",
    "            pred_mask = pred_mask & valid_mask\n",
    "            target_mask = target_mask & valid_mask\n",
    "        \n",
    "        intersection = (pred_mask & target_mask).sum()\n",
    "        union = (pred_mask | target_mask).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(float(intersection) / float(union))\n",
    "    \n",
    "    return np.nanmean(ious)\n",
    "\n",
    "def compute_pixel_accuracy(pred, target, ignore_index=255):\n",
    "    \"\"\"Pixel Accuracy 계산\"\"\"\n",
    "    if ignore_index is not None:\n",
    "        valid_mask = (target != ignore_index)\n",
    "        pred_valid = pred[valid_mask]\n",
    "        target_valid = target[valid_mask]\n",
    "    else:\n",
    "        pred_valid = pred\n",
    "        target_valid = target\n",
    "    \n",
    "    if len(pred_valid) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return float((pred_valid == target_valid).sum()) / len(pred_valid)\n",
    "\n",
    "def compute_attack_success_rate(pred_clean, pred_adv, target, ignore_index=255):\n",
    "    \"\"\"Attack Success Rate 계산\"\"\"\n",
    "    if ignore_index is not None:\n",
    "        valid_mask = (target != ignore_index)\n",
    "        pred_clean_valid = pred_clean[valid_mask]\n",
    "        pred_adv_valid = pred_adv[valid_mask]\n",
    "        target_valid = target[valid_mask]\n",
    "    else:\n",
    "        pred_clean_valid = pred_clean\n",
    "        pred_adv_valid = pred_adv\n",
    "        target_valid = target\n",
    "    \n",
    "    if len(pred_clean_valid) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    correct_clean = (pred_clean_valid == target_valid)\n",
    "    incorrect_adv = (pred_adv_valid != target_valid)\n",
    "    \n",
    "    success = (correct_clean & incorrect_adv).sum()\n",
    "    return float(success) / len(pred_clean_valid)\n",
    "\n",
    "###############################################################################\n",
    "# 7. Main Functions (PGD만 수행)\n",
    "###############################################################################\n",
    "def generate_and_save_pgd_images(model, dataloader, save_dir, **attack_params):\n",
    "    \"\"\"PGD adversarial 이미지들을 생성하고 저장\"\"\"\n",
    "    print(f\"\\n[INFO] Generating PGD adversarial images...\")\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    saved_files = []\n",
    "    \n",
    "    for batch_idx, (images, labels, filenames, original_sizes) in enumerate(tqdm(dataloader, desc=\"Generating PGD\")):\n",
    "        images = images.to(device, dtype=torch.float32)\n",
    "        labels = labels.to(device, dtype=torch.long)\n",
    "        \n",
    "        # Generate adversarial images using PGD\n",
    "        adv_images = pgd_attack(model, images, labels, **attack_params)\n",
    "        \n",
    "        # Save each image in the batch with original size\n",
    "        for i, (filename, original_size) in enumerate(zip(filenames, original_sizes)):\n",
    "            save_path = save_adversarial_image_with_original_size(adv_images[i:i+1], filename, original_size, save_dir)\n",
    "            saved_files.append((filename, save_path))\n",
    "    \n",
    "    print(f\"[INFO] Saved {len(saved_files)} PGD images to {save_dir}\")\n",
    "    return saved_files\n",
    "\n",
    "def evaluate_pgd_images(model, clean_dataloader, adv_save_dir):\n",
    "    \"\"\"저장된 PGD adversarial 이미지들을 평가\"\"\"\n",
    "    print(f\"\\n[INFO] Evaluating PGD adversarial images...\")\n",
    "    \n",
    "    results = {\n",
    "        'pixel_accuracy': [],\n",
    "        'miou': [],\n",
    "        'attack_success_rate': []\n",
    "    }\n",
    "    \n",
    "    for batch_idx, (clean_images, labels, filenames, original_sizes) in enumerate(tqdm(clean_dataloader, desc=\"Evaluating PGD\")):\n",
    "        clean_images = clean_images.to(device, dtype=torch.float32)\n",
    "        labels = labels.to(device, dtype=torch.long)\n",
    "        \n",
    "        # Load adversarial images\n",
    "        adv_images_list = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, filename in enumerate(filenames):\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            adv_filename = base_name + '.jpg'\n",
    "            adv_path = os.path.join(adv_save_dir, adv_filename)\n",
    "            \n",
    "            if os.path.exists(adv_path):\n",
    "                adv_img = Image.open(adv_path).convert(\"RGB\")\n",
    "                adv_img = adv_img.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.BILINEAR)\n",
    "                adv_img = np.array(adv_img, dtype=np.float32)\n",
    "                \n",
    "                # Normalize\n",
    "                mean = np.array([123.675, 116.28, 103.53]).reshape(1, 1, 3)\n",
    "                std = np.array([58.395, 57.12, 57.375]).reshape(1, 1, 3)\n",
    "                adv_img = (adv_img - mean) / std\n",
    "                adv_img = adv_img.transpose(2, 0, 1)\n",
    "                adv_images_list.append(adv_img)\n",
    "                valid_indices.append(i)\n",
    "            else:\n",
    "                print(f\"[WARN] Adversarial image not found: {adv_path}\")\n",
    "        \n",
    "        if not adv_images_list:\n",
    "            continue\n",
    "            \n",
    "        adv_images = torch.stack([torch.from_numpy(img) for img in adv_images_list]).to(device, dtype=torch.float32)\n",
    "        \n",
    "        # Filter clean images and labels for valid indices\n",
    "        clean_images_valid = clean_images[valid_indices]\n",
    "        labels_valid = labels[valid_indices]\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            pred_clean = get_model_output(model, clean_images_valid)\n",
    "            pred_adv = get_model_output(model, adv_images)\n",
    "        \n",
    "        # Calculate metrics for each valid image\n",
    "        for i in range(len(labels_valid)):\n",
    "            pred_clean_np = pred_clean[i].cpu().numpy()\n",
    "            pred_adv_np = pred_adv[i].cpu().numpy()\n",
    "            label_np = labels_valid[i].cpu().numpy()\n",
    "            \n",
    "            # Pixel accuracy\n",
    "            acc = compute_pixel_accuracy(pred_adv_np, label_np, IGNORE_INDEX)\n",
    "            results['pixel_accuracy'].append(acc)\n",
    "            \n",
    "            # mIoU\n",
    "            miou = compute_miou(pred_adv_np, label_np, NUM_CLASSES, IGNORE_INDEX)\n",
    "            results['miou'].append(miou)\n",
    "            \n",
    "            # Attack success rate\n",
    "            asr = compute_attack_success_rate(pred_clean_np, pred_adv_np, label_np, IGNORE_INDEX)\n",
    "            results['attack_success_rate'].append(asr)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_results = {}\n",
    "    for key, values in results.items():\n",
    "        if values:\n",
    "            avg_results[f'avg_{key}'] = np.mean(values)\n",
    "            avg_results[f'std_{key}'] = np.std(values)\n",
    "        else:\n",
    "            avg_results[f'avg_{key}'] = 0.0\n",
    "            avg_results[f'std_{key}'] = 0.0\n",
    "    \n",
    "    return avg_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 함수 - PGD만 수행\"\"\"\n",
    "    # 1. Load model\n",
    "    model = load_deeplabv3_model()\n",
    "    \n",
    "    # 2. Prepare dataloader (10개 이미지로 빠른 테스트)\n",
    "    test_loader = get_dataloader(IMAGE_DIR, MASK_DIR, batch_size=1, max_images=5000)\n",
    "    \n",
    "    # 3. PGD attack parameters\n",
    "    std_tensor = torch.tensor([58.395, 57.12, 57.375]).view(1, 3, 1, 1).to(device)\n",
    "    epsilon = (8.0 / std_tensor)  # 8/255 in normalized space\n",
    "    alpha = (2.0 / std_tensor)    # 2/255 in normalized space\n",
    "    num_steps = 20\n",
    "    \n",
    "    pgd_params = {\n",
    "        \"epsilon\": epsilon,\n",
    "        \"alpha\": alpha,\n",
    "        \"num_steps\": num_steps,\n",
    "        \"ignore_index\": IGNORE_INDEX\n",
    "    }\n",
    "    \n",
    "    # 4. Generate and save PGD adversarial images\n",
    "    pgd_save_dir = os.path.join(ADV_SAVE_DIR, \"pgd\")\n",
    "    saved_files = generate_and_save_pgd_images(model, test_loader, pgd_save_dir, **pgd_params)\n",
    "    \n",
    "    # 5. Evaluate the saved PGD images\n",
    "    results = evaluate_pgd_images(model, test_loader, pgd_save_dir)\n",
    "    \n",
    "    # 6. Add attack info to results\n",
    "    results['attack_name'] = 'PGD'\n",
    "    results['num_images'] = len(saved_files)\n",
    "    results['save_directory'] = pgd_save_dir\n",
    "    \n",
    "    # 7. Print results\n",
    "    print(f\"\\n[RESULT] PGD Results (10 test images):\")\n",
    "    print(f\"  Pixel Accuracy: {results['avg_pixel_accuracy']:.4f} ± {results['std_pixel_accuracy']:.4f}\")\n",
    "    print(f\"  mIoU: {results['avg_miou']:.4f} ± {results['std_miou']:.4f}\")\n",
    "    print(f\"  Attack Success Rate: {results['avg_attack_success_rate']:.4f} ± {results['std_attack_success_rate']:.4f}\")\n",
    "    print(f\"  Number of images saved: {results['num_images']}\")\n",
    "    print(f\"  Save directory: {results['save_directory']}\")\n",
    "    \n",
    "    # 8. Save results to CSV\n",
    "    df = pd.DataFrame([results])\n",
    "    results_csv = os.path.join(ADV_SAVE_DIR, \"deeplabv3_pgd_results_5000imgs.csv\")\n",
    "    df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\n[INFO] Results saved to: {results_csv}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DEEPLABV3 PGD ATTACK RESULTS (5000 TEST IMAGES)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pixel Accuracy: {results['avg_pixel_accuracy']:.4f}\")\n",
    "    print(f\"mIoU: {results['avg_miou']:.4f}\")\n",
    "    print(f\"Attack Success Rate: {results['avg_attack_success_rate']:.4f}\")\n",
    "    print(f\"Images processed: {results['num_images']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02894ddf",
   "metadata": {},
   "source": [
    "### Mask R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef464924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD for MMDetection 3.x (Mask R-CNN, COCO)\n",
    "import os, torch\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# ========= User paths =========\n",
    "CONFIG ='C:/Users/heheh/mmdetection/configs/mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py'\n",
    "CHECKPOINT = 'C:/Users/heheh/mmdetection/checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth'\n",
    "DATA_ROOT = 'C:/Users/heheh/mmdetection/data/coco'\n",
    "ANN_FILE  = os.path.join(DATA_ROOT, 'annotations', 'instances_val2017.json')\n",
    "IMG_DIR   = os.path.join(DATA_ROOT, 'val2017')\n",
    "ADV_SAVE_DIR = os.path.join(DATA_ROOT, 'instance_maskrcnn_adv')\n",
    "os.makedirs(ADV_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ========= Attack hyperparams (pixel-space) =========\n",
    "epsilon = 8/255.0     # L_inf epsilon in [0,1] space\n",
    "alpha   = 2/255.0     # step size in [0,1]\n",
    "num_steps = 20\n",
    "\n",
    "# ========= 1) Build runner/model + dataloader =========\n",
    "cfg = Config.fromfile(CONFIG)\n",
    "cfg.default_scope = 'mmdet'\n",
    "cfg.load_from = CHECKPOINT\n",
    "cfg.work_dir = './work_dirs/pgd_adv_eval'\n",
    "\n",
    "# Point the test dataset to COCO val\n",
    "test_dataset = cfg.test_dataloader.dataset\n",
    "test_dataset.type = 'CocoDataset'\n",
    "test_dataset.data_root = DATA_ROOT\n",
    "test_dataset.ann_file = ANN_FILE\n",
    "test_dataset.data_prefix = dict(img=IMG_DIR)\n",
    "test_dataset.test_mode = True\n",
    "\n",
    "# optional: limit to first N images while debugging\n",
    "# test_dataset.indices = list(range(20))\n",
    "\n",
    "# Evaluator (keep if you want to also report metrics on adv images later)\n",
    "cfg.test_evaluator = dict(type='CocoMetric', ann_file=ANN_FILE, metric=['bbox','segm'])\n",
    "\n",
    "# Visualizer (avoid empty list crash)\n",
    "if hasattr(cfg, 'visualizer'):\n",
    "    cfg.visualizer.vis_backends = None\n",
    "\n",
    "runner = Runner.from_cfg(cfg)\n",
    "model = runner.model\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# ========= 2) Utility: convert pixel-space eps/alpha to normalized space =========\n",
    "# MMDet defaults use mean/std in 0-255 scale.\n",
    "preproc = model.data_preprocessor\n",
    "mean = torch.tensor(preproc.mean, device=device).view(1,3,1,1)      # shape (1,C,1,1)\n",
    "std  = torch.tensor(preproc.std,  device=device).view(1,3,1,1)\n",
    "\n",
    "# bounds in normalized space for [0,255] pixel range:\n",
    "lower = (0.0 - mean) / std\n",
    "upper = (255.0 - mean) / std\n",
    "\n",
    "# pixel-space eps -> normalized-space eps (per channel)\n",
    "eps_norm   = (epsilon * 255.0) / std\n",
    "alpha_norm = (alpha   * 255.0) / std\n",
    "\n",
    "# ========= 3) DataLoader (use same as test) =========\n",
    "test_loader = runner.build_dataloader(cfg.test_dataloader)\n",
    "\n",
    "# ========= 4) PGD loop per image and SAVE =========\n",
    "def total_loss_sum(losses):\n",
    "    \"\"\"Sum all tensors in the nested dict returned by model(..., mode='loss').\"\"\"\n",
    "    total = 0.0\n",
    "    for v in losses.values():\n",
    "        if isinstance(v, dict):\n",
    "            for vv in v.values():\n",
    "                if torch.is_tensor(vv):\n",
    "                    total = total + vv.sum()\n",
    "        elif torch.is_tensor(v):\n",
    "            total = total + v.sum()\n",
    "    return total\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_adv_in_original_size(norm_img, ds, save_path):\n",
    "    \"\"\"\n",
    "    norm_img: (1,3,H_pad,W_pad) normalized tensor ((img-mean)/std) in the model space\n",
    "    ds:       DetDataSample for this image (contains ori/img/pad shapes)\n",
    "    Saves an RGB image at the original size (ori_w, ori_h).\n",
    "    \"\"\"\n",
    "    # 1) denorm back to 0..255\n",
    "    x = norm_img.clone() * std + mean            # (1,3,H,W) in 0..255\n",
    "    x = x.clamp(0, 255)[0].permute(1, 2, 0).cpu().numpy().astype(np.uint8)  # (H,W,3)\n",
    "\n",
    "    # 2) crop padding (keep only real img region)\n",
    "    img_h, img_w = ds.metainfo['img_shape'][:2]   # after resize, before pad\n",
    "    x = x[:img_h, :img_w, :]                      # remove bottom/right pad\n",
    "\n",
    "    # 3) resize back to original size\n",
    "    ori_h, ori_w = ds.metainfo['ori_shape'][:2]\n",
    "    pil = Image.fromarray(x)                      # x is RGB here\n",
    "    pil = pil.resize((ori_w, ori_h), resample=Image.BILINEAR)\n",
    "\n",
    "    # 4) save\n",
    "    pil.save(save_path, quality=95)\n",
    "\n",
    "# Enable grads inside loop\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "for batch in test_loader:\n",
    "    # batch is a dict with 'inputs' (list of tensors) and 'data_samples' (list of DetDataSample)\n",
    "    # Preprocessor already ran in DataLoader collate -> inputs are normalized tensors on device after model.data_preprocessor?\n",
    "    # In test loader, they are raw; we must preprocess using model.data_preprocessor manually:\n",
    "    data = model.data_preprocessor(batch, training=False)  # returns dict with 'inputs' tensor and 'data_samples' list\n",
    "\n",
    "    inputs = data['inputs']            # Tensor (N,C,H,W), normalized with mean/std\n",
    "    data_samples = data['data_samples']  # list of DetDataSample with metainfo set\n",
    "    N = inputs.shape[0]\n",
    "\n",
    "    # Process images one-by-one (simpler to keep filenames, shapes consistent)\n",
    "    for i in range(N):\n",
    "        img = inputs[i:i+1].detach().clone().to(device)  # (1,C,H,W) normalized\n",
    "        ds  = [data_samples[i]]  # list with one sample\n",
    "\n",
    "        # Get a filename to save under\n",
    "        img_name = ds[0].metainfo.get('img_path', None)\n",
    "        if img_name is None:\n",
    "            # fallback to ori_filename if available\n",
    "            img_name = ds[0].metainfo.get('ori_filename', f'idx_{ds[0].metainfo.get(\"img_id\",\"unk\")}.jpg')\n",
    "        base = os.path.splitext(os.path.basename(img_name))[0]\n",
    "        save_path = os.path.join(ADV_SAVE_DIR, f'{base}.jpg')\n",
    "\n",
    "        # Initialize adv as the clean normalized image\n",
    "        x_adv = img.clone().detach()\n",
    "        x_adv.requires_grad_(True)\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Make sure metainfo shapes reflect current tensor size\n",
    "            H, W = x_adv.shape[2], x_adv.shape[3]\n",
    "            ds[0].set_metainfo({\n",
    "                'img_shape': (H, W, 3),\n",
    "                'pad_shape': (H, W, 3),\n",
    "                'batch_input_shape': (H, W)\n",
    "            })\n",
    "\n",
    "            # Zero model grads; keep only x_adv gradient\n",
    "            model.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Forward in \"loss\" mode\n",
    "            losses = model(x_adv, ds, mode='loss')                                                                                                          \n",
    "            loss = total_loss_sum(losses)\n",
    "            loss.backward()\n",
    "\n",
    "            # PGD update in normalized space\n",
    "            with torch.no_grad():\n",
    "                grad = x_adv.grad\n",
    "                x_adv = x_adv + alpha_norm * torch.sign(grad)                       \n",
    "                # Project back to the L_inf ball around original img\n",
    "                x_adv = torch.max(torch.min(x_adv, img + eps_norm), img - eps_norm)\n",
    "                # Clamp to valid normalized range\n",
    "                x_adv = torch.max(torch.min(x_adv, upper), lower)\n",
    "\n",
    "            x_adv.requires_grad_(True)\n",
    "                              \n",
    "        # Save adv image (denormalize to 0..255 and write PNG)\n",
    "        with torch.no_grad():\n",
    "            save_adv_in_original_size(x_adv, ds[0], save_path)\n",
    "\n",
    "        print(f'[PGD] Saved: {save_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4386e4a",
   "metadata": {},
   "source": [
    "### Panoptic FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e48e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD for PanopticFPN (MMDetection 3.x)\n",
    "import os, copy, torch, numpy as np\n",
    "from PIL import Image\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmengine.runner.checkpoint import load_checkpoint\n",
    "\n",
    "# ====== Paths ======\n",
    "CONFIG = r'C:/Users/heheh/mmdetection/configs/panoptic_fpn/panoptic-fpn_r50_fpn_1x_coco.py'\n",
    "CHECKPOINT = r'C:/Users/heheh/mmdetection/checkpoints/panoptic_fpn_r50_fpn_1x_coco_20210821_101153-9668fd13.pth'\n",
    "DATA_ROOT = r'C:/Users/heheh/mmdetection/data/coco'\n",
    "PAN_JSON  = os.path.join(DATA_ROOT, 'annotations', 'panoptic_val2017.json')\n",
    "IMG_DIR   = os.path.join(DATA_ROOT, 'val2017')\n",
    "PAN_SEG   = os.path.join(DATA_ROOT, 'annotations', 'panoptic_val2017')  # PNG dir\n",
    "ADV_SAVE_DIR = os.path.join(DATA_ROOT, 'panoptic_fpn_adv')\n",
    "os.makedirs(ADV_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ====== Attack hyperparams (pixel-space) ======\n",
    "epsilon   = 8/255.0           # L_inf radius\n",
    "alpha     = 2/255.0           # step size\n",
    "num_steps = 20\n",
    "random_start = True           # good for stronger attacks\n",
    "\n",
    "# ====== Build model ======\n",
    "cfg = Config.fromfile(CONFIG)\n",
    "cfg.default_scope = 'mmdet'\n",
    "cfg.load_from = CHECKPOINT\n",
    "cfg.work_dir = './work_dirs/pgd_panoptic'\n",
    "if hasattr(cfg, 'visualizer'):\n",
    "    cfg.visualizer.vis_backends = None\n",
    "\n",
    "runner = Runner.from_cfg(cfg)\n",
    "model = runner.model\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "# Ensure weights are loaded for ad-hoc usage:\n",
    "load_checkpoint(model, CHECKPOINT, map_location=device)\n",
    "model.cfg = cfg  # handy, if you later use APIs that expect .cfg\n",
    "\n",
    "# ====== Build an ATTACK dataloader WITH GT (test_mode=False) ======\n",
    "# Use the training pipeline (loads bboxes/masks/sem seg) but point to VAL panoptic\n",
    "attack_dl_cfg = copy.deepcopy(cfg.train_dataloader)\n",
    "ds = attack_dl_cfg.dataset\n",
    "# If your train_dataloader wraps the dataset (e.g., RepeatDataset), unwrap to the innermost:\n",
    "while hasattr(ds, 'dataset'):\n",
    "    ds = ds.dataset\n",
    "ds.type = 'CocoPanopticDataset'\n",
    "ds.data_root = DATA_ROOT\n",
    "ds.ann_file = PAN_JSON\n",
    "ds.data_prefix = dict(img=IMG_DIR, seg=PAN_SEG)  # img dir + GT panoptic PNG dir\n",
    "ds.test_mode = False  # IMPORTANT: include ground-truth for losses\n",
    "\n",
    "# Keep batch size 1 (simplifies per-image saving & metadata handling)\n",
    "attack_dl_cfg.batch_size = 1\n",
    "attack_dl_cfg.num_workers = 2\n",
    "\n",
    "attack_loader = runner.build_dataloader(attack_dl_cfg)\n",
    "\n",
    "# ====== Normalization bounds (model uses mean/std in 0-255 space) ======\n",
    "pre = model.data_preprocessor\n",
    "mean = torch.tensor(pre.mean, device=device).view(1,3,1,1)\n",
    "std  = torch.tensor(pre.std,  device=device).view(1,3,1,1)\n",
    "lower = (0.0   - mean) / std\n",
    "upper = (255.0 - mean) / std\n",
    "\n",
    "eps_norm   = (epsilon * 255.0) / std\n",
    "alpha_norm = (alpha   * 255.0) / std\n",
    "\n",
    "# ====== Utilities ======\n",
    "def sum_all_losses(losses: dict):\n",
    "    total = 0.0\n",
    "    for v in losses.values():\n",
    "        if isinstance(v, dict):\n",
    "            for t in v.values():\n",
    "                if torch.is_tensor(t):\n",
    "                    total = total + t.sum()\n",
    "        elif torch.is_tensor(v):\n",
    "            total = total + v.sum()\n",
    "    return total\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_adv_in_original_size(norm_img_1CHW, data_sample, save_path):\n",
    "    \"\"\"Denorm -> unpad -> resize back to ori size -> save\"\"\"\n",
    "    x = norm_img_1CHW.clone() * std + mean        # 0..255\n",
    "    x = x.clamp(0,255)[0].permute(1,2,0).cpu().numpy().astype(np.uint8)  # HxWx3\n",
    "\n",
    "    img_h, img_w = data_sample.metainfo['img_shape'][:2]   # after resize, before pad\n",
    "    x = x[:img_h, :img_w, :]                               # remove pad\n",
    "\n",
    "    ori_h, ori_w = data_sample.metainfo['ori_shape'][:2]\n",
    "    Image.fromarray(x).resize((ori_w, ori_h), Image.BILINEAR).save(save_path, quality=95)\n",
    "\n",
    "# ====== PGD loop ======\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "for data_batch in attack_loader:\n",
    "    # Normalize & collate like the runner would\n",
    "    data = model.data_preprocessor(data_batch, training=False)  # {'inputs': tensor, 'data_samples': list}\n",
    "    imgs = data['inputs']             # (1,3,H,W), normalized\n",
    "    samples = data['data_samples']    # [DetDataSample] with GT for panoptic (instances + semantic)\n",
    "\n",
    "    x = imgs.detach().clone().to(device)       # clean normalized\n",
    "    if random_start:\n",
    "        x = x + torch.empty_like(x).uniform_(-1,1) * eps_norm\n",
    "        x = torch.max(torch.min(x, x + eps_norm), x - eps_norm)  # stay in ball\n",
    "        x = torch.max(torch.min(x, upper), lower)                # valid range\n",
    "\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    # Derive a good filename\n",
    "    meta = samples[0].metainfo\n",
    "    base = os.path.splitext(os.path.basename(meta.get('ori_filename', meta.get('img_path', 'img'))))[0]\n",
    "    save_path = os.path.join(ADV_SAVE_DIR, f'{base}.jpg')\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        H, W = x.shape[2], x.shape[3]\n",
    "        samples[0].set_metainfo({\n",
    "            'img_shape': (H, W, 3),\n",
    "            'pad_shape': (H, W, 3),\n",
    "            'batch_input_shape': (H, W),\n",
    "        })\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        losses = model(x, samples, mode='loss')   # PanopticFPN: RPN/ROI + semantic head losses\n",
    "        loss = sum_all_losses(losses)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            grad = x.grad\n",
    "            x = x + alpha_norm * torch.sign(grad)           # untargeted: maximize total loss\n",
    "            x = torch.max(torch.min(x, imgs + eps_norm), imgs - eps_norm)  # project to L_inf ball\n",
    "            x = torch.max(torch.min(x, upper), lower)       # clamp valid range\n",
    "\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        save_adv_in_original_size(x, samples[0], save_path)\n",
    "    print(f'[PGD PanopticFPN] Saved {save_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
