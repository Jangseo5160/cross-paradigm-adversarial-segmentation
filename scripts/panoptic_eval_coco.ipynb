{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "# ---- Base paths (EDIT) ----\n",
    "DATA_ROOT   = r'C:/Users/heheh/mmdetection/data/coco'\n",
    "CONFIG      = r'C:/Users/heheh/mmdetection/configs/panoptic_fpn/panoptic-fpn_r50_fpn_1x_coco.py'\n",
    "CHECKPOINT  = r'C:/Users/heheh/mmdetection/checkpoints/panoptic_fpn_r50_fpn_1x_coco_20210821_101153-9668fd13.pth'\n",
    "\n",
    "IMG_DIR_CLEAN = os.path.join(DATA_ROOT, 'val2017')\n",
    "PAN_JSON  = os.path.join(DATA_ROOT, 'annotations', 'panoptic_val2017.json')\n",
    "PAN_SEG   = os.path.join(DATA_ROOT, 'annotations', 'panoptic_val2017')\n",
    "WORK_DIR  = './work_dirs/compare_multi'\n",
    "\n",
    "# Add as many dirs as you want:\n",
    "ADV_DIRS = {\n",
    "    'deeplabv3_pgd': r'C:/Users/heheh/mmsegmentation/data/semantic_adv_deeplabv3',\n",
    "    'maskrcnn_pgd':  r'C:/Users/heheh/mmdetection/data/coco/instance_maskrcnn_adv',\n",
    "    'panoptic_pgd':  r'C:/Users/heheh/mmdetection/data/coco/panoptic_fpn_adv',\n",
    "}\n",
    "\n",
    "BATCH = 2\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "PREDCHANGE_THRESH = 0.05  # 5% pixels changed => success for ASR-predchange\n",
    "\n",
    "SAMPLE_N = 5000            # evaluate at most 100 images per adv setting\n",
    "SAMPLE_SEED = 42\n",
    "SAMPLE_MODE = 'random'     # 'random' or 'first'\n",
    "PREDCHANGE_THRESH = 0.05   # success if ≥5% pixels change\n",
    "\n",
    "def existing_indices(ann_json, img_root):\n",
    "    with open(ann_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    names = [im['file_name'] for im in data['images']]\n",
    "    idx = [i for i, n in enumerate(names) if os.path.exists(os.path.join(img_root, n))]\n",
    "    return idx, names\n",
    "\n",
    "def pick_sample(indices, n, mode='random', seed=0):\n",
    "    \"\"\"Pick up to n indices deterministically.\"\"\"\n",
    "    if not indices:\n",
    "        return []\n",
    "    if mode == 'first':\n",
    "        return indices[:min(n, len(indices))]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if len(indices) <= n:\n",
    "        return list(indices)\n",
    "    return list(rng.choice(indices, size=n, replace=False))\n",
    "\n",
    "def restrict_to_existing(indices, names, root, n=None, seed=0, mode='random'):\n",
    "    \"\"\"Keep only indices whose image file exists under root; optionally subsample to n.\"\"\"\n",
    "    keep = [i for i in indices if os.path.exists(os.path.join(root, names[i]))]\n",
    "    if n is not None and len(keep) > n:\n",
    "        keep = pick_sample(keep, n, mode=mode, seed=seed)\n",
    "    return keep\n",
    "\n",
    "def build_cfg(img_root, tag):\n",
    "    cfg = Config.fromfile(CONFIG)\n",
    "    cfg.default_scope = 'mmdet'\n",
    "    cfg.load_from = CHECKPOINT\n",
    "    cfg.work_dir = os.path.join(WORK_DIR, tag)\n",
    "    os.makedirs(cfg.work_dir, exist_ok=True)\n",
    "\n",
    "    ds = cfg.test_dataloader.dataset\n",
    "    ds.type = 'CocoPanopticDataset'\n",
    "    ds.data_root = DATA_ROOT\n",
    "    ds.ann_file = PAN_JSON\n",
    "    ds.data_prefix = dict(img=img_root, seg=PAN_SEG)\n",
    "    ds.test_mode = True\n",
    "\n",
    "    cfg.test_dataloader.batch_size = BATCH\n",
    "    cfg.test_dataloader.num_workers = NUM_WORKERS\n",
    "\n",
    "    cfg.test_evaluator = dict(\n",
    "        type='CocoPanopticMetric',\n",
    "        ann_file=PAN_JSON,\n",
    "        seg_prefix=PAN_SEG,\n",
    "        format_only=False\n",
    "    )\n",
    "    if hasattr(cfg, 'visualizer'):\n",
    "        cfg.visualizer.vis_backends = None\n",
    "    return cfg\n",
    "\n",
    "def run_eval(cfg):\n",
    "    runner = Runner.from_cfg(cfg)\n",
    "    return runner.test()  # dict\n",
    "\n",
    "def get_pred_sem_map(ds):\n",
    "    \"\"\"\n",
    "    Return (H,W) int64 semantic category map from a DetDataSample.\n",
    "    Prefers semantic branch; else builds semantic map from panoptic ids + segments_info.\n",
    "    Falls back to INSTANCE_OFFSET decoding if segments_info is missing.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    # 1) Prefer semantic prediction if present\n",
    "    sem = getattr(ds, 'pred_sem_seg', None)\n",
    "    if sem is not None:\n",
    "        seg = getattr(sem, 'sem_seg', None)\n",
    "        if seg is None:\n",
    "            seg = getattr(sem, 'data', None)\n",
    "        if isinstance(seg, torch.Tensor):\n",
    "            if seg.ndim == 3:  # (C,H,W) logits\n",
    "                seg = seg.argmax(dim=0)\n",
    "            seg = seg.detach().cpu().to(torch.int64).numpy()\n",
    "        else:\n",
    "            seg = np.asarray(seg, dtype=np.int64)\n",
    "        return seg\n",
    "\n",
    "    # 2) Panoptic prediction\n",
    "    p = getattr(ds, 'pred_panoptic_seg', None)\n",
    "    if p is None:\n",
    "        return None\n",
    "\n",
    "    # sequentially try fields (NO \"or\" with tensors!)\n",
    "    pan = getattr(p, 'panoptic_seg', None)\n",
    "    if pan is None:\n",
    "        pan = getattr(p, 'sem_seg', None)\n",
    "    if pan is None:\n",
    "        pan = getattr(p, 'data', None)\n",
    "\n",
    "    if pan is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(pan, torch.Tensor):\n",
    "        pan = pan.detach().cpu()\n",
    "    pan = np.asarray(pan)\n",
    "\n",
    "    # segments_info can be on PixelData or in metainfo\n",
    "    segs = getattr(p, 'segments_info', None)\n",
    "    if segs is None:\n",
    "        meta = getattr(p, 'metainfo', None)\n",
    "        if isinstance(meta, dict):\n",
    "            segs = meta.get('segments_info', None)\n",
    "\n",
    "    if segs:\n",
    "        sem_map = np.zeros_like(pan, dtype=np.int64)\n",
    "        for s in segs:\n",
    "            sid = int(s['id'] if isinstance(s, dict) else s.id)\n",
    "            cid = int(s['category_id'] if isinstance(s, dict) else s.category_id)\n",
    "            sem_map[pan == sid] = cid\n",
    "        return sem_map\n",
    "\n",
    "    # 3) Fallback: decode with common INSTANCE_OFFSET convention\n",
    "    INSTANCE_OFFSET = 1000\n",
    "    return (pan % INSTANCE_OFFSET).astype(np.int64)\n",
    "\n",
    "\n",
    "def pull_metric(m, which):  # which in {'PQ','SQ','RQ'}\n",
    "    if which in m:\n",
    "        return m[which]\n",
    "    # try common variants\n",
    "    lk = which.lower()\n",
    "    for k, v in m.items():\n",
    "        kl = k.lower()\n",
    "        if kl == lk or kl.endswith('.' + lk):\n",
    "            # scale if looks like a fraction\n",
    "            return float(v) * 100.0 if (isinstance(v, (int,float)) and v <= 1.5) else float(v)\n",
    "    return float('nan')\n",
    "\n",
    "def _pick_case_insensitive(d, key):\n",
    "    # get d[key] ignoring case; returns None if missing\n",
    "    lk = key.lower()\n",
    "    for k, v in d.items():\n",
    "        if k.lower() == lk:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _scale_if_fraction(x):\n",
    "    # If looks like 0–1, convert to %\n",
    "    return float(x) * 100.0 if isinstance(x, (int, float)) and 0.0 <= x <= 1.5 else float(x)\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def pull_panoptic_all(m):\n",
    "    \"\"\"\n",
    "    Return (PQ_all, SQ_all, RQ_all) from CocoPanopticMetric results across MMDet 3.x variants.\n",
    "    Looks for:\n",
    "      - 'coco_panoptic/PQ' (preferred)\n",
    "      - any key ending with '/PQ', '/SQ', '/RQ'\n",
    "      - avoids *_th, *_st, 'things', 'stuff'\n",
    "      - also handles nested dicts under 'All'\n",
    "    \"\"\"\n",
    "    def pick(key_base):\n",
    "        key_base_l = key_base.lower()\n",
    "        best = None\n",
    "\n",
    "        # 1) direct match like 'coco_panoptic/PQ'\n",
    "        for k, v in m.items():\n",
    "            kl = k.lower()\n",
    "            if kl == f'coco_panoptic/{key_base_l}':\n",
    "                return float(v)\n",
    "\n",
    "        # 2) suffix '/PQ' (and not *_th/_st)\n",
    "        pat = re.compile(rf'(^|[/\\.]){key_base_l}$')\n",
    "        for k, v in m.items():\n",
    "            kl = k.lower()\n",
    "            if any(x in kl for x in ['_th', '_st', 'things', 'stuff']):\n",
    "                continue\n",
    "            if pat.search(kl):\n",
    "                best = float(v)\n",
    "                break\n",
    "\n",
    "        if best is not None:\n",
    "            return best\n",
    "\n",
    "        # 3) nested under 'All': {'All': {'PQ': ...}}\n",
    "        all_block = None\n",
    "        for k, v in m.items():\n",
    "            if k.lower() == 'all' and isinstance(v, dict):\n",
    "                all_block = v\n",
    "                break\n",
    "        if all_block is not None:\n",
    "            for k, v in all_block.items():\n",
    "                if k.lower() == key_base_l and isinstance(v, (int, float)):\n",
    "                    return float(v)\n",
    "\n",
    "        # 4) nested under 'PQ': {'PQ': {'All': ...}}\n",
    "        block = None\n",
    "        for k, v in m.items():\n",
    "            if k.lower() == key_base_l and isinstance(v, dict):\n",
    "                block = v\n",
    "                break\n",
    "        if block is not None:\n",
    "            for k, v in block.items():\n",
    "                if k.lower() == 'all' and isinstance(v, (int, float)):\n",
    "                    return float(v)\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    pq = pick('PQ'); sq = pick('SQ'); rq = pick('RQ')\n",
    "    # (optional) scale 0–1 → %\n",
    "    for name, val in [('PQ', pq), ('SQ', sq), ('RQ', rq)]:\n",
    "        if isinstance(val, (int, float)) and 0.0 <= val <= 1.5:\n",
    "            if name == 'PQ': pq = float(val) * 100.0\n",
    "            if name == 'SQ': sq = float(val) * 100.0\n",
    "            if name == 'RQ': rq = float(val) * 100.0\n",
    "    return pq, sq, rq\n",
    "\n",
    "def pull_all_metrics(m):\n",
    "    \"\"\"\n",
    "    Robustly get (PQ_all, SQ_all, RQ_all) from CocoPanopticMetric outputs across MMDet versions.\n",
    "    Returns floats or NaN.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    pq = sq = rq = np.nan\n",
    "\n",
    "    # 1) direct top-level numbers\n",
    "    if isinstance(_pick_case_insensitive(m, 'PQ'), (int, float)):\n",
    "        pq = _scale_if_fraction(_pick_case_insensitive(m, 'PQ'))\n",
    "    if isinstance(_pick_case_insensitive(m, 'SQ'), (int, float)):\n",
    "        sq = _scale_if_fraction(_pick_case_insensitive(m, 'SQ'))\n",
    "    if isinstance(_pick_case_insensitive(m, 'RQ'), (int, float)):\n",
    "        rq = _scale_if_fraction(_pick_case_insensitive(m, 'RQ'))\n",
    "\n",
    "    # 2) nested under 'PQ'/'SQ'/'RQ' → {'PQ': {'All': ...}, ...}\n",
    "    if np.isnan(pq):\n",
    "        d = _pick_case_insensitive(m, 'PQ')\n",
    "        if isinstance(d, dict):\n",
    "            v = _pick_case_insensitive(d, 'All')\n",
    "            if isinstance(v, (int, float)): pq = _scale_if_fraction(v)\n",
    "    if np.isnan(sq):\n",
    "        d = _pick_case_insensitive(m, 'SQ')\n",
    "        if isinstance(d, dict):\n",
    "            v = _pick_case_insensitive(d, 'All')\n",
    "            if isinstance(v, (int, float)): sq = _scale_if_fraction(v)\n",
    "    if np.isnan(rq):\n",
    "        d = _pick_case_insensitive(m, 'RQ')\n",
    "        if isinstance(d, dict):\n",
    "            v = _pick_case_insensitive(d, 'All')\n",
    "            if isinstance(v, (int, float)): rq = _scale_if_fraction(v)\n",
    "\n",
    "    # 3) nested under 'All' → {'All': {'PQ': ..., 'SQ': ..., 'RQ': ...}}\n",
    "    d_all = _pick_case_insensitive(m, 'All')\n",
    "    if isinstance(d_all, dict):\n",
    "        if np.isnan(pq):\n",
    "            v = _pick_case_insensitive(d_all, 'PQ')\n",
    "            if isinstance(v, (int, float)): pq = _scale_if_fraction(v)\n",
    "        if np.isnan(sq):\n",
    "            v = _pick_case_insensitive(d_all, 'SQ')\n",
    "            if isinstance(v, (int, float)): sq = _scale_if_fraction(v)\n",
    "        if np.isnan(rq):\n",
    "            v = _pick_case_insensitive(d_all, 'RQ')\n",
    "            if isinstance(v, (int, float)): rq = _scale_if_fraction(v)\n",
    "\n",
    "    # 4) flattened keys like 'All.pq'\n",
    "    for k, v in m.items():\n",
    "        if not isinstance(v, (int, float)): \n",
    "            continue\n",
    "        kl = k.lower()\n",
    "        if 'all' in kl and ('.pq' in kl or kl.endswith('pq')) and np.isnan(pq):\n",
    "            pq = _scale_if_fraction(v)\n",
    "        elif 'all' in kl and ('.sq' in kl or kl.endswith('sq')) and np.isnan(sq):\n",
    "            sq = _scale_if_fraction(v)\n",
    "        elif 'all' in kl and ('.rq' in kl or kl.endswith('rq')) and np.isnan(rq):\n",
    "            rq = _scale_if_fraction(v)\n",
    "\n",
    "    return pq, sq, rq\n",
    "\n",
    "def asr_predchange(adv_root, model, subset_names, predchange_thresh=PREDCHANGE_THRESH):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "      - ASR_predchange = successes/N (success if ≥ predchange_thresh of pixels changed)\n",
    "      - avg_changed_pixels = mean fraction changed across N images\n",
    "    Only evaluates images present in adv_root within subset_names.\n",
    "    \"\"\"\n",
    "    names = [n for n in subset_names if os.path.exists(os.path.join(adv_root, n))]\n",
    "    if not names:\n",
    "        return np.nan, 0, np.nan, (np.nan, np.nan), np.nan  # asr, N, mean, CI, std\n",
    "\n",
    "    diffrates = []\n",
    "    successes = 0\n",
    "    for n in tqdm(names, desc=f'ASR-predchange ({os.path.basename(adv_root)})'):\n",
    "        clean_path = os.path.join(IMG_DIR_CLEAN, n)\n",
    "        adv_path   = os.path.join(adv_root, n)\n",
    "\n",
    "        ds_c = inference_detector(model, clean_path)\n",
    "\n",
    "        ds_a = inference_detector(model,   adv_path)\n",
    "        if not hasattr(get_pred_sem_map, \"_once\"):\n",
    "            lc_dbg = get_pred_sem_map(ds_c); la_dbg = get_pred_sem_map(ds_a)\n",
    "            print(\"[dbg] shapes:\", None if lc_dbg is None else lc_dbg.shape,\n",
    "                                None if la_dbg is None else la_dbg.shape)\n",
    "            get_pred_sem_map._once = True\n",
    "            \n",
    "        lc = get_pred_sem_map(ds_c)\n",
    "        la = get_pred_sem_map(ds_a)\n",
    "        if lc is None or la is None:\n",
    "            continue\n",
    "        if lc.shape != la.shape:\n",
    "            # resize adv semantic map to clean shape (nearest)\n",
    "            import cv2\n",
    "            la = cv2.resize(la, (lc.shape[1], lc.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        diff = (lc != la).mean()\n",
    "        diffrates.append(diff)\n",
    "        if diff >= predchange_thresh:\n",
    "            successes += 1\n",
    "\n",
    "    N = len(diffrates)\n",
    "    if N == 0:\n",
    "        return np.nan, 0, np.nan, (np.nan, np.nan), np.nan\n",
    "\n",
    "    asr = successes / N\n",
    "    # 95% Wilson CI for a proportion\n",
    "    z = 1.96\n",
    "    denom = 1 + z**2 / N\n",
    "    center = (asr + z*z/(2*N)) / denom\n",
    "    margin = z * np.sqrt((asr*(1-asr) + z*z/(4*N)) / N) / denom\n",
    "    ci_low, ci_high = max(0.0, center - margin), min(1.0, center + margin)\n",
    "\n",
    "    return asr, N, float(np.mean(diffrates)), (ci_low, ci_high), float(np.std(diffrates))\n",
    "\n",
    "# ---------- Build global name list ----------\n",
    "all_idx_clean, all_names = existing_indices(PAN_JSON, IMG_DIR_CLEAN)\n",
    "# Base candidate pool (images that exist in clean dir)\n",
    "base_pool = all_idx_clean\n",
    "\n",
    "# Build once for inference-based ASR\n",
    "pan_model = init_detector(CONFIG, CHECKPOINT, device=DEVICE)\n",
    "\n",
    "# ---------- 2) Loop all ADV dirs ----------\n",
    "rows = []\n",
    "import pandas as pd\n",
    "\n",
    "_, all_names = existing_indices(PAN_JSON, IMG_DIR_CLEAN)\n",
    "\n",
    "for tag, adv_dir in ADV_DIRS.items():\n",
    "    if not os.path.isdir(adv_dir):\n",
    "        print(f\"[WARN] skip missing dir: {adv_dir}\")\n",
    "        continue\n",
    "     # Pick a 100-image subset that also exists in ADV dir (deterministic)\n",
    "    adv_subset_idx = restrict_to_existing(\n",
    "        indices=pick_sample(base_pool, SAMPLE_N, mode=SAMPLE_MODE, seed=SAMPLE_SEED),\n",
    "        names=all_names,\n",
    "        root=adv_dir,\n",
    "        n=SAMPLE_N, seed=SAMPLE_SEED, mode=SAMPLE_MODE\n",
    "    )\n",
    "    subset_names = [all_names[i] for i in adv_subset_idx]\n",
    "    \n",
    "    # --- Clean metrics on the same subset\n",
    "    cfg_clean = build_cfg(IMG_DIR_CLEAN, f'pan_clean_{tag}')\n",
    "    cfg_clean.test_dataloader.dataset.indices = adv_subset_idx\n",
    "    met_clean = run_eval(cfg_clean)\n",
    "    # --- Adv metrics on the same subset\n",
    "    cfg_adv = build_cfg(adv_dir, f'pan_adv_{tag}')\n",
    "    cfg_adv.test_dataloader.dataset.indices = adv_subset_idx\n",
    "    met_adv = run_eval(cfg_adv)\n",
    "\n",
    "    # --- ASR on the same subset (no GT)\n",
    "    asr, N_asr, avg_diff, (ci_lo, ci_hi), diff_std = asr_predchange(adv_dir, pan_model, subset_names)\n",
    "    \n",
    "    pq_c, sq_c, rq_c = pull_panoptic_all(met_clean)\n",
    "    pq_a, sq_a, rq_a = pull_panoptic_all(met_adv)\n",
    "    \n",
    "    \n",
    "# (B) Build LONG format with split={clean, adv}, then pivot to columns\n",
    "rows_long = []\n",
    "for r in rows:\n",
    "    # CLEAN row\n",
    "    rows_long.append({\n",
    "        'adv_tag': r['adv_tag'],\n",
    "        'split': 'clean',\n",
    "        'PQ': r['PQ_clean'],\n",
    "        'SQ': r['SQ_clean'],\n",
    "        'RQ': r['RQ_clean'],\n",
    "        # ASR is adv-only; keep NaN for clean\n",
    "        'ASR_predchange': np.nan,\n",
    "        'ASR_95CI_low': np.nan,\n",
    "        'ASR_95CI_high': np.nan,\n",
    "        'ASR_N': np.nan,\n",
    "        'avg_changed_pixels': np.nan,\n",
    "        'std_changed_pixels': np.nan,\n",
    "        'thresh': r['thresh'],\n",
    "        'images_eval': r['images_eval'],\n",
    "    })\n",
    "    # ADV row\n",
    "    rows_long.append({\n",
    "        'adv_tag': r['adv_tag'],\n",
    "        'split': 'adv',\n",
    "        'PQ': r['PQ_adv'],\n",
    "        'SQ': r['SQ_adv'],\n",
    "        'RQ': r['RQ_adv'],\n",
    "        'ASR_predchange': r['ASR_predchange'],\n",
    "        'ASR_95CI_low': r['ASR_95CI_low'],\n",
    "        'ASR_95CI_high': r['ASR_95CI_high'],\n",
    "        'ASR_N': r['ASR_N'],\n",
    "        'avg_changed_pixels': r['avg_changed_pixels'],\n",
    "        'std_changed_pixels': r['std_changed_pixels'],\n",
    "        'thresh': r['thresh'],\n",
    "        'images_eval': r['images_eval'],\n",
    "    })\n",
    "\n",
    "df_long = pd.DataFrame(rows_long)\n",
    "\n",
    "# List the value columns we want to pivot\n",
    "value_cols = [\n",
    "    'PQ','SQ','RQ',\n",
    "    'ASR_predchange','ASR_95CI_low','ASR_95CI_high','ASR_N',\n",
    "    'avg_changed_pixels','std_changed_pixels','thresh','images_eval'\n",
    "]\n",
    "\n",
    "# Pivot so we get columns like PQ_clean, PQ_adv, ...\n",
    "df_wide = df_long.pivot(index='adv_tag', columns='split', values=value_cols)\n",
    "\n",
    "# Add delta columns for PQ, SQ, RQ\n",
    "for m in ['PQ', 'SQ', 'RQ']:\n",
    "    df_wide[(m, 'delta')] = df_wide[(m, 'adv')] - df_wide[(m, 'clean')]\n",
    "\n",
    "# Flatten MultiIndex columns -> \"PQ_clean\", \"PQ_adv\", \"PQ_delta\", ...\n",
    "df_wide.columns = [f'{a}_{b}' for a, b in df_wide.columns]\n",
    "df_wide = df_wide.reset_index()\n",
    "\n",
    "# (optional) reorder columns\n",
    "ordered = (\n",
    "    ['adv_tag', 'images_eval_clean'] +\n",
    "    [f'{m}_{s}' for m in ['PQ','SQ','RQ'] for s in ['clean','adv','delta']] +\n",
    "    ['ASR_predchange_adv','ASR_95CI_low_adv','ASR_95CI_high_adv','ASR_N_adv',\n",
    "     'avg_changed_pixels_adv','std_changed_pixels_adv','thresh_adv']\n",
    ")\n",
    "# Keep only those that exist\n",
    "ordered = [c for c in ordered if c in df_wide.columns]\n",
    "df_wide = df_wide[ordered]\n",
    "\n",
    "out_csv_wide = f'./panoptic_transfer_subset{SAMPLE_N}_seed{SAMPLE_SEED}_clean_as_columns.csv'\n",
    "df_wide.to_csv(out_csv_wide, index=False)\n",
    "\n",
    "# Pretty print (optional)\n",
    "def f3(x): \n",
    "    return \"—\" if (x is None or (isinstance(x, float) and np.isnan(x))) else f\"{x:.3f}\"\n",
    "\n",
    "print(\"\\n=== PanopticFPN — results (clean/adv as columns) ===\")\n",
    "show_cols = [c for c in df_wide.columns if c != 'adv_tag']\n",
    "print(df_wide[['adv_tag'] + show_cols].to_string(index=False, formatters={c: f3 for c in show_cols}))\n",
    "print(f'\\n[Saved] {out_csv_wide}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
